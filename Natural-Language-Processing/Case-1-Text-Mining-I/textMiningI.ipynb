{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Text Mining\n",
    "\n",
    "a. Aplikasi yang dibangun harus memanfaatkan sejumlah teknik pemrosesan teks, \n",
    "termasuk tokenisasi, penghapusan kata umum (stop words), stemming atau \n",
    "lemmatize, POS (Part-of-Speech) Tagging, NER (Named Entity Recognition), \n",
    "distribusi frekuensi, pengambilan korpora dari data NLTK atau situs web, \n",
    "pemanfaatan WordNet, ekstraksi fitur, klasifikasi menggunakan metode Na√Øve \n",
    "Bayes, serta kemampuan untuk menyimpan dan memuat model klasifikasi.\n",
    "<br><br>\n",
    "b. Model yang dihasilkan dari arsitektur Na√Øve Bayes harus mencapai tingkat akurasi \n",
    "minimal sebesar 80%.\n",
    "<br><br>\n",
    "c. Aplikasi juga diharapkan mampu menampilkan 5 most informative features dari \n",
    "dataset yang digunakan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "import joblib\n",
    "import contractions\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Necessary NLTK Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   S. No.                                       Message_body     Label\n",
      "0       1                         Rofl. Its true to its name  Non-Spam\n",
      "1       2  The guy did some bitching but I acted like i'd...  Non-Spam\n",
      "2       3  Pity, * was in mood for that. So...any other s...  Non-Spam\n",
      "3       4               Will √º b going to esplanade fr home?  Non-Spam\n",
      "4       5  This is the 2nd time we have tried 2 contact u...      Spam\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv('SMS_train.csv', encoding='ISO-8859-1')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek apakah ada NaN atau None dalam kolom Message_body\n",
    "\n",
    "df['Message_body'].isnull().sum()  # Akan memberi tahu jika ada nilai yang hilang\n",
    "\n",
    "\n",
    "# Jika ada, bisa menghapus baris yang mengandung NaN\n",
    "\n",
    "df = df.dropna(subset=['Message_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Text (Stopwords, Stemming, and Lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Preprocess\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Tokenisasi\n",
    "\n",
    "    tokens = word_tokenize(text.lower())  # Mengubah teks menjadi huruf kecil dan tokenisasi\n",
    "\n",
    "    # Menghapus tanda baca\n",
    "\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Stopwords\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Stemming\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    s = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Lemmatizing\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    l = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # return \" \".join(tokens)\n",
    "    return \" \".join(s + l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "\n",
    "df['processed_text'] = df['Message_body'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction using Bag of Words model\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Target variable\n",
    "\n",
    "y = df['Label'].apply(lambda x: 1 if x == 'Spam' else 0)  # 1 = Spam, 0 = Non-Spam\n",
    "\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize and train Naive Bayes classifier\n",
    "\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.58%\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion (5 Informative Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 94.10%\n",
      "\n",
      "5 Fitur Paling Informatif:\n",
      "\n",
      "('aah', -8.411388132519262)\n",
      "('peach', -8.411388132519262)\n",
      "('paul', -8.411388132519262)\n",
      "('patrick', -8.411388132519262)\n",
      "('patient', -8.411388132519262)\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Misalkan df['processed_text'] berisi teks yang sudah diproses dan df['Label'] berisi label (Spam/Non-Spam)\n",
    "\n",
    "\n",
    "X = df['processed_text']\n",
    "y = df['Label']\n",
    "\n",
    "\n",
    "# Split data menjadi train dan test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Vectorisasi data teks\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Membuat dan melatih model Naive Bayes\n",
    "\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "\n",
    "# Menghitung akurasi\n",
    "\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Akurasi: {accuracy*100:.2f}%\\n\")\n",
    "\n",
    "\n",
    "# Menampilkan 5 fitur paling informatif\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "sorted_coef_index = nb_classifier.feature_log_prob_[1].argsort()  # Menggunakan feature_log_prob_ untuk MultinomialNB\n",
    "\n",
    "top_features = [(feature_names[i], nb_classifier.feature_log_prob_[1][i]) for i in sorted_coef_index[:5]]\n",
    "\n",
    "\n",
    "# Printing\n",
    "\n",
    "\n",
    "print(\"5 Fitur Paling Informatif:\\n\")\n",
    "\n",
    "# print(top_features)\n",
    "\n",
    "for feature in top_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Explanation</b>\n",
    "\n",
    "Akurasi = (Jumlah Prediksi Benar) / (Jumlah Total Prediksi)\n",
    "\n",
    "#### Struktur output:\n",
    "\n",
    "**[('aah', -8.411388132519262), ('peach', -8.411388132519262) ...]**:\n",
    "- Setiap item dalam list tersebut adalah sebuah tuple yang berisi **(fitur, koefisien)**.\n",
    "- **Fitur** adalah kata atau istilah yang ditemukan dalam teks, yang berperan dalam pembuatan prediksi. Dalam hal ini, fitur adalah kata-kata seperti `'aah'`, `'patrick'`, `'patient'`, dsb.\n",
    "- **Koefisien** adalah nilai yang menunjukkan **seberapa besar kontribusi fitur tersebut** terhadap prediksi. Koefisien ini diperoleh dari model Naive Bayes.\n",
    "\n",
    "Koefisien yang negatif menunjukkan bahwa kata tersebut lebih sering muncul dalam pesan-pesan Spam dibandingkan dengan pesan-pesan Non-Spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to a file\n",
    "\n",
    "joblib.dump(nb_classifier, 'spam_classifier_model.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model and vectorizer later\n",
    "\n",
    "model = joblib.load('spam_classifier_model.pkl')\n",
    "vectorizer = joblib.load('vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                        rofl true name rofl true name\n",
      "1    guy bitch act like interest buy someth els nex...\n",
      "2               piti mood suggest pity mood suggestion\n",
      "3    √º b go esplanad fr home √º b going esplanade fr...\n",
      "4    time tri contact u pound prize claim easi call...\n",
      "Name: processed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['processed_text'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nlp(sentences):\n",
    "\n",
    "    preprocessed_words = []\n",
    "    preprocessed_sentences = []\n",
    "    \n",
    "    # Tokenizing\n",
    "    # Membagi teks ke dalam suatu token (word, symbol)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sentence = contractions.fix(sentence.replace(\"\\n\", \". \").replace(\". .\", '. '))\n",
    "        \n",
    "        # Make all lowercase\n",
    "\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        list_sentences = sent_tokenize(sentence)\n",
    "        list_word = word_tokenize(sentence)\n",
    "        \n",
    "        # Remove unneccessary symbol\n",
    "\n",
    "        list_word = [re.sub(r\"[^a-zA-Z0-9\\s/.]\", '', word) for word in list_word]\n",
    "        list_word = list(filter(None, list_word))\n",
    "        \n",
    "        # Stop words\n",
    "\n",
    "        eng_stopwords = stopwords.words(\"english\")\n",
    "        list_word = [word for word in list_word if word.lower() not in eng_stopwords]\n",
    "        \n",
    "        # Remove punctuation\n",
    "\n",
    "        list_word = [word for word in list_word if word not in string.punctuation]\n",
    "        \n",
    "\n",
    "        # Stemming\n",
    "\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        snowball_stemmer = SnowballStemmer(\"english\")\n",
    "        lancaster_stemmer = LancasterStemmer()\n",
    "        \n",
    "        list_word = [lancaster_stemmer.stem(word) for word in list_word]\n",
    "        \n",
    "\n",
    "        # Lemmatizing\n",
    "\n",
    "        wnl = WordNetLemmatizer()\n",
    "        # list_word = [wnl.lemmatize(word, pos='r') for word in list_word]\n",
    "        # print(list_word)\n",
    "        \n",
    "\n",
    "        # POS Tagging\n",
    "\n",
    "        tagged = pos_tag(list_word)\n",
    "\n",
    "        # print(tagged)\n",
    "        \n",
    "        # Named Entity Recognition\n",
    "\n",
    "        ner = ne_chunk(tagged)\n",
    "            \n",
    "        for w in list_word:\n",
    "            preprocessed_words.append(w)\n",
    "        \n",
    "        for s in list_sentences:\n",
    "            preprocessed_sentences.append(s)\n",
    "    return preprocessed_sentences, preprocessed_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Notes\n",
    "\n",
    "- PorterStemmer() - Algoritma lambat, akurasi bagus\n",
    "- SnowballStemmer(\"english\") - PorterStemmer V2 (dalam beberapa bahasa)\n",
    "- LancasterStemmer() - Algoritmanya cepat, akurasi kurang bagus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          rofl true name rofl true name\n",
       "1      guy bitch act like interest buy someth els nex...\n",
       "2                 piti mood suggest pity mood suggestion\n",
       "3      √º b go esplanad fr home √º b going esplanade fr...\n",
       "4      time tri contact u pound prize claim easi call...\n",
       "                             ...                        \n",
       "952    how favourit person today r u workin hard coul...\n",
       "953                     much got clean much got cleaning\n",
       "954    sorri da gone mad mani pend work sorry da gone...\n",
       "955                  wat time √º finish wat time √º finish\n",
       "956                               glad talk glad talking\n",
       "Name: processed_text, Length: 957, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senteces and Words:\n",
      "['rofl true name rofl true name', 'guy bitch act like interest buy someth els next week gave us free guy bitching acted like interested buying something else next week gave you free', 'piti mood suggest pity mood suggestion', '√º b go esplanad fr home √º b going esplanade fr home', 'time tri contact you pound prize claim easi call per minut time tried contact you pound prize claim easy call per minute', 'remind get pound free call credit detail great offer pl repli text valid name hous postcod reminder get pound free call credit detail great offer pls reply text valid name house postcode', 'huh lei huh lei', 'wait least wednesday see get wait least wednesday see get', 'ard like dat lor ard like dat lor', 'ok lor soni ericsson salesman ask shuhui say quit gd use consid ok lor sony ericsson salesman ask shuhui say quite gd use considering']\n",
      "['rofl', 'tru', 'nam', 'rofl', 'tru', 'nam', 'guy', 'bitch', 'act', 'lik', 'interest', 'buy', 'some', 'el', 'next', 'week', 'gav', 'us', 'fre', 'guy', 'bitch', 'act', 'lik', 'interest', 'buy', 'someth', 'els', 'next', 'week', 'gav', 'fre', 'pit', 'mood', 'suggest', 'pity', 'mood', 'suggest', 'b', 'go', 'esplanad', 'fr', 'hom', 'b', 'going', 'esplanad', 'fr', 'hom', 'tim', 'tri', 'contact', 'pound', 'priz', 'claim', 'eas', 'cal', 'per', 'minut', 'tim', 'tri', 'contact', 'pound', 'priz', 'claim', 'easy', 'cal', 'per', 'minut', 'remind', 'get', 'pound', 'fre', 'cal', 'credit', 'detail', 'gre', 'off', 'pl', 'repl', 'text', 'valid', 'nam', 'hou', 'postcod', 'remind', 'get', 'pound', 'fre', 'cal', 'credit', 'detail', 'gre', 'off', 'pls', 'reply', 'text', 'valid', 'nam', 'hous', 'postcod', 'huh', 'lei', 'huh', 'lei', 'wait', 'least', 'wednesday', 'see', 'get', 'wait', 'least', 'wednesday', 'see', 'get', 'ard', 'lik', 'dat', 'lor', 'ard', 'lik', 'dat', 'lor', 'ok', 'lor', 'son', 'ericsson', 'salesm', 'ask', 'shuhu', 'say', 'quit', 'gd', 'us', 'consid', 'ok', 'lor', 'sony', 'ericsson', 'salesm', 'ask', 'shuhu', 'say', 'quit', 'gd', 'us', 'consid']\n"
     ]
    }
   ],
   "source": [
    "# Frequency Distribution\n",
    "\n",
    "ps1, pw1 = preprocess_nlp(df['processed_text'][:10]) # Hanya mengambil 10 row, karena prosesnya agak lambat\n",
    "print(\"Senteces and Words:\")\n",
    "print(ps1)\n",
    "print(pw1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Distribution (word, frequency):\n",
      "nam: 4\n",
      "lik: 4\n",
      "fre: 4\n",
      "pound: 4\n",
      "cal: 4\n",
      "get: 4\n",
      "lor: 4\n",
      "us: 3\n",
      "rofl: 2\n",
      "tru: 2\n"
     ]
    }
   ],
   "source": [
    "fd1 = FreqDist(pw1)\n",
    "print(f\"Frequency Distribution (word, frequency):\")\n",
    "for word, count in fd1.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. NER, POS Tagging, and WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER, POS Tagging, and WordNet Results:\n",
      "\n",
      "\n",
      "Analyzing: wen get spiritu deep great wen get spiritual deep great\n",
      "\n",
      "Named Entities: [('wen', 'PERSON')]\n",
      "\n",
      "POS Tags: [('wen', 'NN'), ('get', 'VB'), ('spiritu', 'JJ'), ('deep', 'JJ'), ('great', 'JJ'), ('wen', 'JJ'), ('get', 'VB'), ('spiritual', 'JJ'), ('deep', 'JJ'), ('great', 'JJ')]\n",
      "\n",
      "WORDNET\n",
      "\n",
      "Synonyms for 'wen': ['steatocystoma', 'sebaceous_cyst', 'pilar_cyst', 'wen']\n",
      "Synonyms for 'get': ['fetch', 'nonplus', 'dumbfound', 'induce', 'grow', 'sustain', 'suffer', 'mother', 'incur', 'buzz_off', 'flummox', 'convey', 'acquire', 'pay_off', 'have', 'pose', 'set_out', 'father', 'contract', 'scram', 'generate', 'stick', 'obtain', 'aim', 'stupefy', 'pay_back', 'beat', 'let', 'start', 'engender', 'develop', \"get_under_one's_skin\", 'beget', 'baffle', 'come', 'make', 'mystify', 'vex', 'catch', 'start_out', 'cause', 'bring', 'find', 'amaze', 'bring_forth', 'set_about', 'experience', 'puzzle', 'commence', 'take', 'fix', 'stimulate', 'capture', 'drive', 'perplex', 'arrive', 'bewilder', 'gravel', 'get_down', 'get', 'arrest', 'begin', 'sire', 'produce', 'go', 'become', 'fuck_off', 'receive', 'draw', 'bugger_off']\n",
      "Synonyms for 'deep': ['mysterious', 'abstruse', 'rich', 'deep', 'recondite', 'cryptical', 'late', 'thick', 'deeply', 'trench', 'bass', 'cryptic', 'inscrutable', 'oceanic_abyss', 'mystifying']\n",
      "Synonyms for 'great': ['swell', 'bully', 'cracking', 'large', 'outstanding', 'majuscule', 'nifty', 'not_bad', 'corking', 'dandy', 'big', 'capital', 'enceinte', 'heavy', 'expectant', 'groovy', 'smashing', 'gravid', 'bang-up', 'with_child', 'peachy', 'great', 'slap-up', 'neat', 'keen']\n",
      "Synonyms for 'wen': ['steatocystoma', 'sebaceous_cyst', 'pilar_cyst', 'wen']\n",
      "Synonyms for 'get': ['fetch', 'nonplus', 'dumbfound', 'induce', 'grow', 'sustain', 'suffer', 'mother', 'incur', 'buzz_off', 'flummox', 'convey', 'acquire', 'pay_off', 'have', 'pose', 'set_out', 'father', 'contract', 'scram', 'generate', 'stick', 'obtain', 'aim', 'stupefy', 'pay_back', 'beat', 'let', 'start', 'engender', 'develop', \"get_under_one's_skin\", 'beget', 'baffle', 'come', 'make', 'mystify', 'vex', 'catch', 'start_out', 'cause', 'bring', 'find', 'amaze', 'bring_forth', 'set_about', 'experience', 'puzzle', 'commence', 'take', 'fix', 'stimulate', 'capture', 'drive', 'perplex', 'arrive', 'bewilder', 'gravel', 'get_down', 'get', 'arrest', 'begin', 'sire', 'produce', 'go', 'become', 'fuck_off', 'receive', 'draw', 'bugger_off']\n",
      "Synonyms for 'spiritual': ['religious', 'phantasmal', 'apparitional', 'unearthly', 'spectral', 'spiritual', 'ghostlike', 'ghostly', 'Negro_spiritual']\n",
      "Synonyms for 'deep': ['mysterious', 'abstruse', 'rich', 'deep', 'recondite', 'cryptical', 'late', 'thick', 'deeply', 'trench', 'bass', 'cryptic', 'inscrutable', 'oceanic_abyss', 'mystifying']\n",
      "Synonyms for 'great': ['swell', 'bully', 'cracking', 'large', 'outstanding', 'majuscule', 'nifty', 'not_bad', 'corking', 'dandy', 'big', 'capital', 'enceinte', 'heavy', 'expectant', 'groovy', 'smashing', 'gravid', 'bang-up', 'with_child', 'peachy', 'great', 'slap-up', 'neat', 'keen']\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Analyzing: cool time think get cool time think get\n",
      "\n",
      "Named Entities: []\n",
      "\n",
      "POS Tags: [('cool', 'JJ'), ('time', 'NN'), ('think', 'VBP'), ('get', 'VB'), ('cool', 'JJ'), ('time', 'NN'), ('think', 'VBP'), ('get', 'VB')]\n",
      "\n",
      "WORDNET\n",
      "\n",
      "Synonyms for 'cool': ['assuredness', 'chill', 'cool_off', 'nerveless', 'cool', 'aplomb', 'cool_down', 'coolheaded', 'poise', 'sang-froid']\n",
      "Synonyms for 'time': ['clock_time', 'metre', 'meter', 'clip', 'prison_term', 'fourth_dimension', 'clock', 'sentence', 'time']\n",
      "Synonyms for 'think': ['consider', 'recall', 'imagine', 'recollect', 'believe', 'call_up', 'intend', 'remember', 'cogitate', 'call_back', 'mean', 'retrieve', 'think', 'opine', 'reckon', 'cerebrate', 'guess', 'suppose', 'conceive']\n",
      "Synonyms for 'get': ['fetch', 'nonplus', 'dumbfound', 'induce', 'grow', 'sustain', 'suffer', 'mother', 'incur', 'buzz_off', 'flummox', 'convey', 'acquire', 'pay_off', 'have', 'pose', 'set_out', 'father', 'contract', 'scram', 'generate', 'stick', 'obtain', 'aim', 'stupefy', 'pay_back', 'beat', 'let', 'start', 'engender', 'develop', \"get_under_one's_skin\", 'beget', 'baffle', 'come', 'make', 'mystify', 'vex', 'catch', 'start_out', 'cause', 'bring', 'find', 'amaze', 'bring_forth', 'set_about', 'experience', 'puzzle', 'commence', 'take', 'fix', 'stimulate', 'capture', 'drive', 'perplex', 'arrive', 'bewilder', 'gravel', 'get_down', 'get', 'arrest', 'begin', 'sire', 'produce', 'go', 'become', 'fuck_off', 'receive', 'draw', 'bugger_off']\n",
      "Synonyms for 'cool': ['assuredness', 'chill', 'cool_off', 'nerveless', 'cool', 'aplomb', 'cool_down', 'coolheaded', 'poise', 'sang-froid']\n",
      "Synonyms for 'time': ['clock_time', 'metre', 'meter', 'clip', 'prison_term', 'fourth_dimension', 'clock', 'sentence', 'time']\n",
      "Synonyms for 'think': ['consider', 'recall', 'imagine', 'recollect', 'believe', 'call_up', 'intend', 'remember', 'cogitate', 'call_back', 'mean', 'retrieve', 'think', 'opine', 'reckon', 'cerebrate', 'guess', 'suppose', 'conceive']\n",
      "Synonyms for 'get': ['fetch', 'nonplus', 'dumbfound', 'induce', 'grow', 'sustain', 'suffer', 'mother', 'incur', 'buzz_off', 'flummox', 'convey', 'acquire', 'pay_off', 'have', 'pose', 'set_out', 'father', 'contract', 'scram', 'generate', 'stick', 'obtain', 'aim', 'stupefy', 'pay_back', 'beat', 'let', 'start', 'engender', 'develop', \"get_under_one's_skin\", 'beget', 'baffle', 'come', 'make', 'mystify', 'vex', 'catch', 'start_out', 'cause', 'bring', 'find', 'amaze', 'bring_forth', 'set_about', 'experience', 'puzzle', 'commence', 'take', 'fix', 'stimulate', 'capture', 'drive', 'perplex', 'arrive', 'bewilder', 'gravel', 'get_down', 'get', 'arrest', 'begin', 'sire', 'produce', 'go', 'become', 'fuck_off', 'receive', 'draw', 'bugger_off']\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Analyzing: know wot peopl wear shirt jumper hat belt know r cribb know wot people wear shirt jumper hat belt know r cribbs\n",
      "\n",
      "Named Entities: []\n",
      "\n",
      "POS Tags: [('know', 'VB'), ('wot', 'NN'), ('peopl', 'NN'), ('wear', 'VBP'), ('shirt', 'JJ'), ('jumper', 'NN'), ('hat', 'WP'), ('belt', 'VBD'), ('know', 'VBP'), ('r', 'NN'), ('cribb', 'NN'), ('know', 'VBP'), ('wot', 'JJ'), ('people', 'NNS'), ('wear', 'VBP'), ('shirt', 'JJ'), ('jumper', 'NN'), ('hat', 'WP'), ('belt', 'VBD'), ('know', 'VBP'), ('r', 'NN'), ('cribbs', 'NN')]\n",
      "\n",
      "WORDNET\n",
      "\n",
      "Synonyms for 'know': ['cognize', 'recognise', 'fuck', 'sleep_together', 'love', 'bang', 'acknowledge', 'sleep_with', 'eff', 'screw', 'be_intimate', 'make_out', 'recognize', 'jazz', 'have_intercourse', 'cognise', 'have_a_go_at_it', 'get_it_on', 'get_laid', 'do_it', 'have_it_away', 'experience', 'have_it_off', 'lie_with', 'know', 'hump', 'bonk', 'bed', 'have_sex', 'make_love', 'live', 'roll_in_the_hay']\n",
      "Synonyms for 'wear': ['clothing', 'wear', 'get_into', 'fag_out', 'have_on', 'endure', 'bust', 'break', 'wear_down', 'wearing', 'jade', 'bear', 'weary', 'wear_upon', 'tire', 'wear_off', 'fatigue', 'hold_out', 'don', 'wear_thin', 'vesture', 'put_on', 'wear_out', 'fall_apart', 'wearable', 'assume', 'outwear', 'tire_out', 'fag', 'habiliment', 'article_of_clothing']\n",
      "Synonyms for 'shirt': ['shirt']\n",
      "Synonyms for 'jumper': ['jump_shot', 'pinafore', 'pinny', 'jumper', 'sweater']\n",
      "Synonyms for 'hat': ['hat', 'lid', 'chapeau']\n",
      "Synonyms for 'belt': ['belt_out', 'belt', 'smash', 'bang', 'knock', 'belted_ammunition', 'swath', 'rap', 'whang', 'bash', 'whack', 'belt_ammunition']\n",
      "Synonyms for 'know': ['cognize', 'recognise', 'fuck', 'sleep_together', 'love', 'bang', 'acknowledge', 'sleep_with', 'eff', 'screw', 'be_intimate', 'make_out', 'recognize', 'jazz', 'have_intercourse', 'cognise', 'have_a_go_at_it', 'get_it_on', 'get_laid', 'do_it', 'have_it_away', 'experience', 'have_it_off', 'lie_with', 'know', 'hump', 'bonk', 'bed', 'have_sex', 'make_love', 'live', 'roll_in_the_hay']\n",
      "Synonyms for 'r': ['gas_constant', 'R', 'radius', 'r', 'roentgen', 'universal_gas_constant']\n",
      "Synonyms for 'know': ['cognize', 'recognise', 'fuck', 'sleep_together', 'love', 'bang', 'acknowledge', 'sleep_with', 'eff', 'screw', 'be_intimate', 'make_out', 'recognize', 'jazz', 'have_intercourse', 'cognise', 'have_a_go_at_it', 'get_it_on', 'get_laid', 'do_it', 'have_it_away', 'experience', 'have_it_off', 'lie_with', 'know', 'hump', 'bonk', 'bed', 'have_sex', 'make_love', 'live', 'roll_in_the_hay']\n",
      "Synonyms for 'people': ['the_great_unwashed', 'multitude', 'people', 'masses', 'mass', 'hoi_polloi', 'citizenry']\n",
      "Synonyms for 'wear': ['clothing', 'wear', 'get_into', 'fag_out', 'have_on', 'endure', 'bust', 'break', 'wear_down', 'wearing', 'jade', 'bear', 'weary', 'wear_upon', 'tire', 'wear_off', 'fatigue', 'hold_out', 'don', 'wear_thin', 'vesture', 'put_on', 'wear_out', 'fall_apart', 'wearable', 'assume', 'outwear', 'tire_out', 'fag', 'habiliment', 'article_of_clothing']\n",
      "Synonyms for 'shirt': ['shirt']\n",
      "Synonyms for 'jumper': ['jump_shot', 'pinafore', 'pinny', 'jumper', 'sweater']\n",
      "Synonyms for 'hat': ['hat', 'lid', 'chapeau']\n",
      "Synonyms for 'belt': ['belt_out', 'belt', 'smash', 'bang', 'knock', 'belted_ammunition', 'swath', 'rap', 'whang', 'bash', 'whack', 'belt_ammunition']\n",
      "Synonyms for 'know': ['cognize', 'recognise', 'fuck', 'sleep_together', 'love', 'bang', 'acknowledge', 'sleep_with', 'eff', 'screw', 'be_intimate', 'make_out', 'recognize', 'jazz', 'have_intercourse', 'cognise', 'have_a_go_at_it', 'get_it_on', 'get_laid', 'do_it', 'have_it_away', 'experience', 'have_it_off', 'lie_with', 'know', 'hump', 'bonk', 'bed', 'have_sex', 'make_love', 'live', 'roll_in_the_hay']\n",
      "Synonyms for 'r': ['gas_constant', 'R', 'radius', 'r', 'roentgen', 'universal_gas_constant']\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Analyzing: tri weekend v trying weekend v\n",
      "\n",
      "Named Entities: [('weekend v trying weekend', 'DATE')]\n",
      "\n",
      "POS Tags: [('tri', 'JJ'), ('weekend', 'NN'), ('v', 'NN'), ('trying', 'VBG'), ('weekend', 'NN'), ('v', 'NN')]\n",
      "\n",
      "WORDNET\n",
      "\n",
      "Synonyms for 'weekend': ['weekend']\n",
      "Synonyms for 'v': ['v', 'five', 'atomic_number_23', 'Phoebe', 'quint', 'quintuplet', 'Little_Phoebe', 'quintet', 'fivesome', '5', 'pentad', 'volt', 'cinque', 'V', 'vanadium', 'fin']\n",
      "Synonyms for 'trying': ['sample', 'stressful', 'try', 'nerve-racking', 'taste', 'test', 'render', 'nerve-wracking', 'examine', 'essay', 'prove', 'seek', 'judge', 'strain', 'assay', 'try_on', 'attempt', 'adjudicate', 'trying', 'hear', 'try_out', 'stress']\n",
      "Synonyms for 'weekend': ['weekend']\n",
      "Synonyms for 'v': ['v', 'five', 'atomic_number_23', 'Phoebe', 'quint', 'quintuplet', 'Little_Phoebe', 'quintet', 'fivesome', '5', 'pentad', 'volt', 'cinque', 'V', 'vanadium', 'fin']\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Analyzing: ic lotta childporn car ic lotta childporn car\n",
      "\n",
      "Named Entities: [('lotta', 'GPE')]\n",
      "\n",
      "POS Tags: [('ic', 'NN'), ('lotta', 'NN'), ('childporn', 'NN'), ('car', 'NN'), ('ic', 'NN'), ('lotta', 'NN'), ('childporn', 'NN'), ('car', 'NN')]\n",
      "\n",
      "WORDNET\n",
      "\n",
      "Synonyms for 'ic': ['IC', 'ic', 'National_Intelligence_Community', '99', 'Intelligence_Community', 'United_States_Intelligence_Community', 'ninety-nine']\n",
      "Synonyms for 'car': ['railway_car', 'auto', 'elevator_car', 'cable_car', 'railroad_car', 'railcar', 'motorcar', 'machine', 'car', 'automobile', 'gondola']\n",
      "Synonyms for 'ic': ['IC', 'ic', 'National_Intelligence_Community', '99', 'Intelligence_Community', 'United_States_Intelligence_Community', 'ninety-nine']\n",
      "Synonyms for 'car': ['railway_car', 'auto', 'elevator_car', 'cable_car', 'railroad_car', 'railcar', 'motorcar', 'machine', 'car', 'automobile', 'gondola']\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model for NER\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# NER (Named Entity Recognition) menggunakan spaCy\n",
    "\n",
    "\n",
    "def perform_ner(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "\n",
    "# POS Tagging menggunakan NLTK\n",
    "\n",
    "\n",
    "def perform_pos_tagging(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "\n",
    "# WordNet untuk sinonim dan antonim\n",
    "\n",
    "\n",
    "def get_wordnet_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())  # Menambahkan sinonim\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "# Menampilkan hasil NER, POS Tagging dan WordNet untuk beberapa contoh kalimat\n",
    "\n",
    "print(\"NER, POS Tagging, and WordNet Results:\\n\")\n",
    "\n",
    "for sentence in df['processed_text'][20:25]:\n",
    "\n",
    "    print(f\"\\nAnalyzing: {sentence}\")\n",
    "    \n",
    "    # NER (Named Entity Recognition)\n",
    "\n",
    "    entities = perform_ner(sentence)\n",
    "    print(\"\\nNamed Entities:\", entities)\n",
    "    print(\"\")\n",
    "    \n",
    "    # POS Tagging\n",
    "\n",
    "    pos_tags = perform_pos_tagging(sentence)\n",
    "    print(\"POS Tags:\", pos_tags)\n",
    "    \n",
    "    # WordNet (Synonyms)\n",
    "\n",
    "    print(\"\\nWORDNET\\n\")\n",
    "\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        synonyms = get_wordnet_synonyms(word)\n",
    "        if synonyms:\n",
    "            print(f\"Synonyms for '{word}': {synonyms}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 Criteria\n",
    "\n",
    "- 1#Classification with Naive Bayes: Accuracy already more than 80% (Done)\n",
    "- 1#Frequency Distribution: Done\n",
    "- 1#NER: Done\n",
    "- 1#POS Tagging: Done\n",
    "- 1#Stemming and Lemmatizing: Done\n",
    "- 1#Text Preprocessing: Done\n",
    "- 1#WordNet: Done\n",
    "\n",
    "<code>Made by: NW25-1</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
